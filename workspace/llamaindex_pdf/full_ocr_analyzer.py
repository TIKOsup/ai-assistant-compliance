# full_ocr_analyzer.py
import pymupdf4llm
import pymupdf
import pytesseract
from pdf2image import convert_from_path
import sys
import os
import argparse
from datetime import datetime
from PIL import Image
import tempfile
import json
import warnings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —Ä–∞–∑–º–µ—Ä–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
warnings.filterwarnings("ignore", category=UserWarning)
Image.MAX_IMAGE_PIXELS = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
os.environ['OPENAI_API_KEY'] = ''

try:
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.llms.ollama import Ollama
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    Settings.llm = Ollama(model="lowl/t-lite", request_timeout=360.0)
    
    LLAMA_INDEX_AVAILABLE = True
    print("‚úÖ LlamaIndex –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏")
    
except ImportError as e:
    print(f"‚ö†Ô∏è LlamaIndex –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    LLAMA_INDEX_AVAILABLE = False

class OCRContractAnalyzer:
    def __init__(self, regulations_path="./regulations", model_name="owl/t-lite"):
        self.regulations_path = regulations_path
        self.regulations_texts = {}
        self.model_name = model_name
        
    def extract_text_with_ocr(self, pdf_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR –¥–ª—è –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print(f"üîç OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(pdf_path)}")
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            print("üì∑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            images = convert_from_path(pdf_path, dpi=300, thread_count=4)
            
            all_text = []
            
            for i, image in enumerate(images):
                print(f"üî§ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i+1}/{len(images)}...")
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
                custom_config = r'--oem 3 --psm 6 -l rus+eng'
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                text = pytesseract.image_to_string(image, config=custom_config)
                
                if text.strip():
                    all_text.append(f"=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1} ===\n{text}")
                    print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                else:
                    print(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}: —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            combined_text = "\n\n".join(all_text)
            print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return combined_text
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
            return None

    def smart_extract_text(self, pdf_path):
        """–£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ - —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π —Å–ø–æ—Å–æ–±, –ø–æ—Ç–æ–º OCR"""
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑: {os.path.basename(pdf_path)}")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        try:
            text = pymupdf4llm.to_markdown(pdf_path)
            if len(text.strip()) > 100:  # –ï—Å–ª–∏ –∏–∑–≤–ª–µ–∫–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                print(f"‚úÖ –û–±—ã—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                print("‚ö†Ô∏è –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –æ–±—ã—á–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR...")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—ã—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            print("üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ OCR...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OCR
        return self.extract_text_with_ocr(pdf_path)

    def load_regulations(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã - —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ, –ø–æ—Ç–æ–º –∏—Å—Ö–æ–¥–Ω—ã–µ"""
        processed_path = "./processed_regulations"
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if os.path.exists(processed_path):
            print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤...")
            txt_files = [f for f in os.listdir(processed_path) if f.endswith('.txt')]
            
            if txt_files:
                print(f"üìö –ù–∞–π–¥–µ–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(txt_files)}")
                
                for file in txt_files:
                    file_path = os.path.join(processed_path, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            text = f.read()
                        
                        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞
                        if "=" * 60 in text:
                            text = text.split("=" * 60, 1)[-1].strip()
                        
                        original_name = file.replace('.txt', '')
                        self.regulations_texts[original_name] = text
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {original_name} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file}: {e}")
                
                if self.regulations_texts:
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.regulations_texts)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤")
                    return True
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ
        if not os.path.exists(self.regulations_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ {self.regulations_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            print("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python universal_processor.py")
            return False
        
        print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤...")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç—å: python universal_processor.py")
        
        files = [f for f in os.listdir(self.regulations_path) 
                if f.lower().endswith(('.pdf', '.docx', '.doc', '.txt', '.xlsx', '.xml'))]
        
        if not files:
            print("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ!")
            return False
        
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
        
        for file in files:
            file_path = os.path.join(self.regulations_path, file)
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file}")
            
            try:
                if file.lower().endswith('.pdf'):
                    text = self.smart_extract_text(file_path)
                elif file.lower().endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                else:
                    text = f"[–§–∞–π–ª {file} —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ universal_processor.py]"
                
                if text and len(text) > 50:
                    self.regulations_texts[file] = text
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {file} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                else:
                    print(f"‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª–µ: {file}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.regulations_texts)} —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤")
        return len(self.regulations_texts) > 0

    def analyze_with_keywords(self, contract_text):
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        print("üîç –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º
        sanctions_keywords = [
            '—Å–∞–Ω–∫—Ü–∏', '–∑–∞–ø—Ä–µ—Ç', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω', '–±–ª–æ–∫–∏—Ä', '–∑–∞–º–æ—Ä–æ–∂',
            '—á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫', '–ø–µ—Ä—Å–æ–Ω–∞ –Ω–æ–Ω –≥—Ä–∞—Ç–∞', '—ç–º–±–∞—Ä–≥–æ'
        ]
        
        currency_keywords = [
            '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '—Ñ—É–Ω—Ç', '—é–∞–Ω—å', '–≤–∞–ª—é—Ç', '–¥–µ–≤–∏–∑',
            '–∫—É—Ä—Å –≤–∞–ª—é—Ç', '–≤–∞–ª—é—Ç–Ω', '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç'
        ]
        
        contract_keywords = [
            '–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω', '—Å—Ç–æ—Ä–æ–Ω', '–ø–æ–∫—É–ø–∞—Ç–µ–ª',
            '–ø—Ä–æ–¥–∞–≤–µ—Ü', '–ø–æ—Å—Ç–∞–≤—â–∏–∫', '–∑–∞–∫–∞–∑—á–∏–∫', '–ø–æ–¥—Ä—è–¥—á–∏–∫'
        ]
        
        risk_keywords = [
            '–æ—Ä—É–∂–∏–µ', '–≤–æ–µ–Ω–Ω', '–¥–≤–æ–π–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏',
            '–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ', '–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ', '—è–¥–µ—Ä–Ω'
        ]
        
        text_lower = contract_text.lower()
        
        # –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        found_sanctions = [kw for kw in sanctions_keywords if kw in text_lower]
        found_currency = [kw for kw in currency_keywords if kw in text_lower]
        found_contract = [kw for kw in contract_keywords if kw in text_lower]
        found_risks = [kw for kw in risk_keywords if kw in text_lower]
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = {
            'is_contract': len(found_contract) > 0,
            'has_sanctions_mentions': len(found_sanctions) > 0,
            'has_currency_operations': len(found_currency) > 0,
            'has_risk_items': len(found_risks) > 0,
            'found_keywords': {
                'sanctions': found_sanctions,
                'currency': found_currency,
                'contract': found_contract,
                'risks': found_risks
            }
        }
        
        return analysis

    def analyze_with_llama(self, contract_text):
        """–ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LlamaIndex"""
        if not LLAMA_INDEX_AVAILABLE:
            print("‚ö†Ô∏è LlamaIndex –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
            return None
        
        print(f"ü§ñ –ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º {self.model_name}...")
        
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–æ–ª–µ–µ –ª–µ–≥–∫–æ–π –º–æ–¥–µ–ª–∏
            Settings.llm = Ollama(
                model=self.model_name, 
                request_timeout=180.0,
                temperature=0.1,
                num_ctx=4096,  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                num_predict=512  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏
            regulations_context = "\n\n".join([
                f"=== {filename} ===\n{text[:2000]}"  # –°–∏–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞
                for filename, text in list(self.regulations_texts.items())[:5]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤
            ])
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_prompt = f"""
–†–ï–ì–õ–ê–ú–ï–ù–¢–´ (–∫—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä):
{regulations_context[:3000]}

–î–û–ì–û–í–û–†:
{contract_text[:]}

–ó–ê–î–ê–ß–ê: –ë—ã—Å—Ç—Ä–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–≥–æ–≤–æ—Ä:
1. –†–ï–®–ï–ù–ò–ï: –ü–†–ò–ù–Ø–¢–¨/–û–¢–ö–ê–ó–ê–¢–¨
2. –ü–†–ò–ß–ò–ù–ê: –≥–ª–∞–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ —Ä–µ—à–µ–Ω–∏—è
3. –†–ò–°–ö–ò: –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º.
"""
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
            doc = Document(text=analysis_prompt)
            index = VectorStoreIndex.from_documents([doc])
            
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑
            query_engine = index.as_query_engine(
                similarity_top_k=2,  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                response_mode="compact"  # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–µ–∂–∏–º
            )
            
            response = query_engine.query(
                "–î–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –æ –¥–æ–≥–æ–≤–æ—Ä–µ"
            )
            
            return str(response)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å: ollama pull llama3.2:3b")
            return None

    def generate_report(self, contract_path, contract_text, keyword_analysis, llm_analysis=None):
        """–°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç"""
        report = {
            'contract_file': os.path.basename(contract_path),
            'analysis_date': datetime.now().isoformat(),
            'text_length': len(contract_text),
            'extraction_method': 'OCR' if 'OCR' in contract_text else 'Standard',
            'keyword_analysis': keyword_analysis,
            'llm_analysis': llm_analysis,
            'recommendation': '–¢–†–ï–ë–£–ï–¢_–ü–†–û–í–ï–†–ö–ò'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        if keyword_analysis['has_sanctions_mentions'] or keyword_analysis['has_risk_items']:
            report['recommendation'] = '–û–¢–ö–ê–ó–ê–¢–¨'
            report['reason'] = '–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–∞–Ω–∫—Ü–∏–π –∏–ª–∏ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ–≤—ã—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å–∫–∞'
        elif keyword_analysis['is_contract'] and keyword_analysis['has_currency_operations']:
            report['recommendation'] = '–¢–†–ï–ë–£–ï–¢_–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô_–ü–†–û–í–ï–†–ö–ò'
            report['reason'] = '–î–æ–≥–æ–≤–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª—é—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞'
        elif keyword_analysis['is_contract']:
            report['recommendation'] = '–ú–û–ñ–ù–û_–†–ê–°–°–ú–û–¢–†–ï–¢–¨'
            report['reason'] = '–ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞'
    def create_summary_report(self, report, summary_file):
        """–°–æ–∑–¥–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç"""
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("–°–í–û–î–ù–´–ô –û–¢–ß–ï–¢ –ê–ù–ê–õ–ò–ó–ê –î–û–ì–û–í–û–†–ê\n")
            f.write("=" * 50 + "\n\n")
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            f.write(f"–§–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞: {report['contract_file']}\n")
            f.write(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {report['analysis_date']}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {report['text_length']} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {report['extraction_method']}\n")
            f.write(f"–§–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º: {report.get('contract_text_file', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n\n")
            
            # –†–µ—à–µ–Ω–∏–µ
            f.write("–†–ï–®–ï–ù–ò–ï –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø\n")
            f.write("-" * 30 + "\n")
            f.write(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {report['recommendation']}\n")
            f.write(f"–ü—Ä–∏—á–∏–Ω–∞: {report.get('reason', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}\n\n")
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            kw = report['keyword_analysis']
            f.write("–ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í\n")
            f.write("-" * 30 + "\n")
            f.write(f"–Ø–≤–ª—è–µ—Ç—Å—è –¥–æ–≥–æ–≤–æ—Ä–æ–º: {'–î–∞' if kw['is_contract'] else '–ù–µ—Ç'}\n")
            f.write(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–∞–Ω–∫—Ü–∏–π: {'–î–∞' if kw['has_sanctions_mentions'] else '–ù–µ—Ç'}\n")
            f.write(f"–í–∞–ª—é—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {'–î–∞' if kw['has_currency_operations'] else '–ù–µ—Ç'}\n")
            f.write(f"–¢–æ–≤–∞—Ä—ã —Ä–∏—Å–∫–∞: {'–î–∞' if kw['has_risk_items'] else '–ù–µ—Ç'}\n\n")
            
            # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            found = kw['found_keywords']
            if any(found.values()):
                f.write("–ù–ê–ô–î–ï–ù–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê\n")
                f.write("-" * 30 + "\n")
                for category, words in found.items():
                    if words:
                        f.write(f"{category.upper()}: {', '.join(words)}\n")
                f.write("\n")
            
            # LLM –∞–Ω–∞–ª–∏–∑
            if report.get('llm_analysis'):
                f.write("–ê–ù–ê–õ–ò–ó –ò–°–ö–£–°–°–¢–í–ï–ù–ù–û–ì–û –ò–ù–¢–ï–õ–õ–ï–ö–¢–ê\n")
                f.write("-" * 30 + "\n")
                f.write(f"{report['llm_analysis']}\n\n")
            
            # –ü—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞
            if report.get('contract_preview'):
                f.write("–ü–†–ï–í–¨–Æ –¢–ï–ö–°–¢–ê –î–û–ì–û–í–û–†–ê\n")
                f.write("-" * 30 + "\n")
                f.write(f"{report['contract_preview']}\n")
                if report['text_length'] > 1000:
                    f.write(f"\n[–ü–æ–∫–∞–∑–∞–Ω–æ 1000 –∏–∑ {report['text_length']} —Å–∏–º–≤–æ–ª–æ–≤]\n")
                f.write("\n")
            
            f.write("=" * 50 + "\n")
            f.write("–ö–æ–Ω–µ—Ü –æ—Ç—á–µ—Ç–∞\n")

    def analyze_contract(self, contract_path):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞"""
        print(f"\n{'='*60}")
        print(f"üìã –ê–ù–ê–õ–ò–ó –î–û–ì–û–í–û–†–ê: {os.path.basename(contract_path)}")
        print(f"{'='*60}")
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        contract_text = self.smart_extract_text(contract_path)
        if not contract_text:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞")
            return None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        contract_name = os.path.splitext(os.path.basename(contract_path))[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞
        contract_text_file = f"contract_text_{contract_name}_{timestamp}.txt"
        with open(contract_text_file, 'w', encoding='utf-8') as f:
            f.write(f"–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢ –î–û–ì–û–í–û–†–ê\n")
            f.write(f"{'='*50}\n")
            f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {os.path.basename(contract_path)}\n")
            f.write(f"–î–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {datetime.now().isoformat()}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {'OCR' if '–°—Ç—Ä–∞–Ω–∏—Ü–∞' in contract_text else '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π'}\n")
            f.write(f"{'='*50}\n\n")
            f.write(contract_text)
        
        print(f"üíæ –¢–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {contract_text_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤)
        print(f"\nüìÑ –ü–†–ï–í–¨–Æ –¢–ï–ö–°–¢–ê –î–û–ì–û–í–û–†–ê:")
        print("-" * 50)
        preview_text = contract_text[:1000]
        print(preview_text)
        if len(contract_text) > 1000:
            print("...")
            print(f"[–ü–æ–∫–∞–∑–∞–Ω–æ {len(preview_text)} –∏–∑ {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤]")
        print("-" * 50)
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_analysis = self.analyze_with_keywords(contract_text)
        
        # 3. LLM –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        llm_analysis = None
        if LLAMA_INDEX_AVAILABLE and self.regulations_texts:
            try:
                llm_analysis = self.analyze_with_llama(contract_text)
            except Exception as e:
                print(f"‚ö†Ô∏è LLM –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                llm_analysis = f"–û—à–∏–±–∫–∞ LLM: {str(e)}"
        
        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫)
        try:
            report = self.generate_report(contract_path, contract_text, keyword_analysis, llm_analysis)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            report = {
                'contract_file': os.path.basename(contract_path),
                'analysis_date': timestamp,
                'text_length': len(contract_text),
                'extraction_method': 'OCR' if '–°—Ç—Ä–∞–Ω–∏—Ü–∞' in contract_text else '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π',
                'keyword_analysis': keyword_analysis,
                'llm_analysis': llm_analysis,
                'recommendation': '–¢–†–ï–ë–£–ï–¢_–ü–†–û–í–ï–†–ö–ò',
                'reason': '–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏–∑–∞'
            }
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ report –Ω–µ None
        if report is None:
            report = {
                'contract_file': os.path.basename(contract_path),
                'analysis_date': timestamp,
                'text_length': len(contract_text),
                'extraction_method': 'OCR' if '–°—Ç—Ä–∞–Ω–∏—Ü–∞' in contract_text else '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π',
                'keyword_analysis': keyword_analysis,
                'llm_analysis': llm_analysis,
                'recommendation': '–û–®–ò–ë–ö–ê_–ê–ù–ê–õ–ò–ó–ê',
                'reason': '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç'
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –≤ –æ—Ç—á–µ—Ç
        report['contract_text_file'] = contract_text_file
        report['contract_preview'] = preview_text
        
        # 5. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        try:
            self.print_analysis_results(report)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥
            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            print(f"üìÑ –§–∞–π–ª: {report['contract_file']}")
            print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {report.get('recommendation', '–ù–ï_–û–ü–†–ï–î–ï–õ–ï–ù–û')}")
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        try:
            report_file = f"contract_analysis_{contract_name}_{timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        
        # 7. –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        try:
            summary_file = f"contract_summary_{contract_name}_{timestamp}.txt"
            self.create_summary_report(report, summary_file)
            print(f"üìã –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")
        
        return report

    def print_analysis_results(self, report):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print("=" * 50)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        rec = report['recommendation']
        if rec == '–û–¢–ö–ê–ó–ê–¢–¨':
            print(f"‚ùå –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: {rec}")
        elif rec == '–ú–û–ñ–ù–û_–†–ê–°–°–ú–û–¢–†–ï–¢–¨':
            print(f"‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: {rec}")
        else:
            print(f"‚ö†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: {rec}")
        
        print(f"üìù –û–ë–û–°–ù–û–í–ê–ù–ò–ï: {report.get('reason', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        kw = report['keyword_analysis']
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
        print(f"  –î–æ–≥–æ–≤–æ—Ä: {'‚úÖ' if kw['is_contract'] else '‚ùå'}")
        print(f"  –°–∞–Ω–∫—Ü–∏–∏: {'‚ö†Ô∏è' if kw['has_sanctions_mentions'] else '‚úÖ'}")
        print(f"  –í–∞–ª—é—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {'‚ö†Ô∏è' if kw['has_currency_operations'] else '‚úÖ'}")
        print(f"  –¢–æ–≤–∞—Ä—ã —Ä–∏—Å–∫–∞: {'‚ùå' if kw['has_risk_items'] else '‚úÖ'}")
        
        # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        found = kw['found_keywords']
        if any(found.values()):
            print(f"\nüîç –ù–ê–ô–î–ï–ù–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:")
            for category, words in found.items():
                if words:
                    print(f"  {category}: {words}")
        
        # LLM –∞–Ω–∞–ª–∏–∑
        if report.get('llm_analysis'):
            print(f"\nü§ñ –ê–ù–ê–õ–ò–ó –ò–°–ö–£–°–°–¢–í–ï–ù–ù–û–ì–û –ò–ù–¢–ï–õ–õ–ï–ö–¢–ê:")
            print("-" * 30)
            print(report['llm_analysis'])

def main(pdf_file):
    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å OCR –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π")
    parser.add_argument("pdf_file", help="–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É –¥–æ–≥–æ–≤–æ—Ä–∞")
    parser.add_argument("--regulations", default="./regulations", help="–ü–∞–ø–∫–∞ —Å —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
    
    args = parser.parse_args()
    args.pdf_file = os.path.abspath(pdf_file)
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å OCR")
    print(f"üìÅ –§–∞–π–ª: {args.pdf_file}")
    print(f"üìö –†–µ–≥–ª–∞–º–µ–Ω—Ç—ã: {args.regulations}")
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = OCRContractAnalyzer(regulations_path=args.regulations)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã
    print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤...")
    if not analyzer.load_regulations():
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã, –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–≥–æ–≤–æ—Ä
    result = analyzer.analyze_contract(args.pdf_file)
    
    if result:
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {result['recommendation']}")
        return result['recommendation']
    else:
        print(f"\n‚ùå –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–∞–º–∏")
        sys.exit(1)

if __name__ == "__main__":
    main()

__all__ = ['main']  