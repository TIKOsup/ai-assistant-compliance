# enhanced_langchain_analyzer.py
import os
import sys
import argparse
from datetime import datetime
import json
import warnings
from pathlib import Path

# PDF –∏ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
import pymupdf4llm
import pytesseract
from pdf2image import convert_from_path
from PIL import Image

# LangChain –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from langchain.llms import Ollama
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")
Image.MAX_IMAGE_PIXELS = None

class EnhancedContractAnalyzer:
    def __init__(self, regulations_path="./regulations", model_name="qwen2.5:3b-instruct"):
        self.regulations_path = regulations_path
        self.model_name = model_name
        self.llm = None
        self.embeddings = None
        self.vectorstore = None
        self.regulations_summary = ""  # –°–≤–æ–¥–∫–∞ –≤—Å–µ—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model_name}")
        
        self.setup_embeddings()
        self.prepare_regulations_summary()
        self.setup_llm()
    
    def prepare_regulations_summary(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–≤–æ–¥–∫—É –≤—Å–µ—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        print("üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–≤–æ–¥–∫–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤...")
        
        processed_path = "./processed_regulations"
        regulations_data = []
        
        if os.path.exists(processed_path):
            txt_files = [f for f in os.listdir(processed_path) if f.endswith('.txt')]
            
            for file in txt_files:
                file_path = os.path.join(processed_path, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –û—á–∏—â–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if "=" * 60 in content:
                        content = content.split("=" * 60, 1)[-1].strip()
                    
                    if len(content.strip()) > 100:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        key_info = self.extract_key_info(content, file)
                        regulations_data.append(key_info)
                        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file}")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file}: {e}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        self.regulations_summary = self.create_regulations_summary(regulations_data)
        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ —Å–≤–æ–¥–∫–∞ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤: {len(self.regulations_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    def extract_key_info(self, content, filename):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        key_info = {
            'filename': filename.replace('.txt', ''),
            'key_points': [],
            'sanctions': [],
            'currency_rules': [],
            'prohibited_items': [],
            'procedures': []
        }
        
        content_lower = content.lower()
        lines = content.split('\n')
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        for line in lines:
            line_clean = line.strip()
            if len(line_clean) < 10:
                continue
                
            line_lower = line_clean.lower()
            
            # –°–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            if any(word in line_lower for word in ['—Å–∞–Ω–∫—Ü–∏', '–∑–∞–ø—Ä–µ—Ç', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω', '–±–ª–æ–∫–∏—Ä', '–∑–∞–º–æ—Ä–æ–∂']):
                key_info['sanctions'].append(line_clean)
            
            # –í–∞–ª—é—Ç–Ω–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            elif any(word in line_lower for word in ['–≤–∞–ª—é—Ç', '–∫—É—Ä—Å', '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç', '–¥–µ–≤–∏–∑']):
                key_info['currency_rules'].append(line_clean)
            
            # –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
            elif any(word in line_lower for word in ['–¥–≤–æ–π–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è', '–æ—Ä—É–∂–∏', '–≤–æ–µ–Ω–Ω', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏']):
                key_info['prohibited_items'].append(line_clean)
            
            # –ü—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            elif any(word in line_lower for word in ['—Ç—Ä–µ–±—É–µ—Ç—Å—è', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '–æ–±—è–∑–∞–Ω', '–¥–æ–ª–∂–µ–Ω']):
                key_info['procedures'].append(line_clean)
            
            # –°—Ç–∞—Ç—å–∏ –∏ –ø—É–Ω–∫—Ç—ã
            elif any(pattern in line_lower for pattern in ['—Å—Ç–∞—Ç—å—è', '–ø—É–Ω–∫—Ç', '—á–∞—Å—Ç—å', '–ø–æ–¥–ø—É–Ω–∫—Ç']):
                key_info['key_points'].append(line_clean)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for key in key_info:
            if isinstance(key_info[key], list):
                key_info[key] = key_info[key][:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        
        return key_info
    
    def create_regulations_summary(self, regulations_data):
        """–°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–≤–æ–¥–∫—É —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤"""
        summary_parts = []
        
        summary_parts.append("=== –ù–û–†–ú–ê–¢–ò–í–ù–ê–Ø –ë–ê–ó–ê ===")
        
        # –°–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
        sanctions_info = []
        for reg in regulations_data:
            if reg['sanctions']:
                sanctions_info.extend(reg['sanctions'])
        
        if sanctions_info:
            summary_parts.append("\nüö® –°–ê–ù–ö–¶–ò–û–ù–ù–´–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:")
            for item in sanctions_info[:10]:  # –¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö
                summary_parts.append(f"‚Ä¢ {item}")
        
        # –í–∞–ª—é—Ç–Ω–æ–µ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        currency_info = []
        for reg in regulations_data:
            if reg['currency_rules']:
                currency_info.extend(reg['currency_rules'])
        
        if currency_info:
            summary_parts.append("\nüí∞ –í–ê–õ–Æ–¢–ù–û–ï –†–ï–ì–£–õ–ò–†–û–í–ê–ù–ò–ï:")
            for item in currency_info[:10]:
                summary_parts.append(f"‚Ä¢ {item}")
        
        # –ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        prohibited_info = []
        for reg in regulations_data:
            if reg['prohibited_items']:
                prohibited_info.extend(reg['prohibited_items'])
        
        if prohibited_info:
            summary_parts.append("\n‚öóÔ∏è –¢–û–í–ê–†–´ –î–í–û–ô–ù–û–ì–û –ù–ê–ó–ù–ê–ß–ï–ù–ò–Ø:")
            for item in prohibited_info[:8]:
                summary_parts.append(f"‚Ä¢ {item}")
        
        # –ü—Ä–æ—Ü–µ–¥—É—Ä—ã
        procedures_info = []
        for reg in regulations_data:
            if reg['procedures']:
                procedures_info.extend(reg['procedures'])
        
        if procedures_info:
            summary_parts.append("\nüìã –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–†–û–¶–ï–î–£–†–´:")
            for item in procedures_info[:8]:
                summary_parts.append(f"‚Ä¢ {item}")
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        summary_parts.append("\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
        for reg in regulations_data:
            summary_parts.append(f"‚Ä¢ {reg['filename']}")
        
        return "\n".join(summary_parts)
    
    def setup_embeddings(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print("‚úÖ –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
    
    def setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM —Å –ø–æ–ª–Ω–æ–π –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–æ–π –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ"""
        print(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ {self.model_name} —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞—Ö
            system_prompt = f"""–¢—ã - –≤–µ–¥—É—â–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ø—Ä–∞–≤—É –∏ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–§.

{self.regulations_summary}

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–≥–æ–≤–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –í–´–®–ï–£–ö–ê–ó–ê–ù–ù–û–ô –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã. 
–í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –∏ —Å—Ç–∞—Ç—å–∏ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–°–¢–†–£–ö–¢–£–†–ê –ê–ù–ê–õ–ò–ó–ê:
üéØ –†–ï–®–ï–ù–ò–ï: [–ü–†–ò–ù–Ø–¢–¨/–û–¢–ö–ê–ó–ê–¢–¨/–¢–†–ï–ë–£–ï–¢_–ü–†–û–í–ï–†–ö–ò]
üìã –û–ë–û–°–ù–û–í–ê–ù–ò–ï: [—Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏–∑ –±–∞–∑—ã]
‚ö†Ô∏è –†–ò–°–ö–ò: [—Ä–∏—Å–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º]
üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò: [–¥–µ–π—Å—Ç–≤–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞–º]
üìö –ò–°–¢–û–ß–ù–ò–ö–ò: [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã]

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–π –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã."""
            
            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
            
            self.llm = Ollama(
                model=self.model_name,
                callback_manager=callback_manager,
                temperature=0.1,
                num_ctx=8192,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤
                num_predict=1024,
                top_k=40,
                top_p=0.9,
                repeat_penalty=1.1,
                system=system_prompt  # –í—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –≤ —Å–∏—Å—Ç–µ–º—É
            )
            
            print("‚úÖ LLM –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM: {e}")
            sys.exit(1)
    
    def extract_text_with_ocr(self, pdf_path):
        """OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        print(f"üîç OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞: {os.path.basename(pdf_path)}")
        
        try:
            images = convert_from_path(pdf_path, dpi=200, thread_count=4)
            all_text = []
            
            for i, image in enumerate(images):
                print(f"üî§ OCR —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}/{len(images)}...")
                custom_config = r'--oem 3 --psm 6 -l rus+eng'
                text = pytesseract.image_to_string(image, config=custom_config)
                
                if text.strip():
                    all_text.append(f"=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1} ===\n{text}")
                del image
            
            combined_text = "\n\n".join(all_text)
            print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return combined_text
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
            return None
    
    def smart_extract_text(self, pdf_path):
        """–£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        try:
            text = pymupdf4llm.to_markdown(pdf_path)
            if len(text.strip()) > 100:
                print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                print("‚ö†Ô∏è –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR...")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        
        return self.extract_text_with_ocr(pdf_path)
    
    def analyze_contract_direct(self, contract_text):
        """–ü—Ä—è–º–æ–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞ LLM —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏"""
        print("ü§ñ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ:

–¢–ï–ö–°–¢ –î–û–ì–û–í–û–†–ê:
{contract_text[:6000]}  # –ü–µ—Ä–≤—ã–µ 6000 —Å–∏–º–≤–æ–ª–æ–≤

–û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï:
1. –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∞ –ø–æ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–º —Å–ø–∏—Å–∫–∞–º
2. –û—Ü–µ–Ω–∏ –≤–∞–ª—é—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞–ª—é—Ç–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É  
3. –ü—Ä–æ–≤–µ—Ä—å —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
4. –û–ø—Ä–µ–¥–µ–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º

–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–≤–æ–µ–π —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.
"""
        
        try:
            response = self.llm(analysis_prompt)
            return response
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
    
    def analyze_contract(self, contract_path):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞"""
        print(f"\n{'='*80}")
        print(f"‚öñÔ∏è  –ê–ù–ê–õ–ò–ó –î–û–ì–û–í–û–†–ê –° –í–°–¢–†–û–ï–ù–ù–´–ú–ò –†–ï–ì–õ–ê–ú–ï–ù–¢–ê–ú–ò")
        print(f"üìÑ –§–∞–π–ª: {os.path.basename(contract_path)}")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model_name}")
        print(f"üìö –†–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ: {len(self.regulations_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"{'='*80}")
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞
        contract_text = self.smart_extract_text(contract_path)
        if not contract_text:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞")
            return None
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        contract_name = Path(contract_path).stem
        
        text_file = f"contract_text_{contract_name}_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(f"–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢ –î–û–ì–û–í–û–†–ê\n")
            f.write(f"{'='*60}\n")
            f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {os.path.basename(contract_path)}\n")
            f.write(f"–î–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {datetime.now().isoformat()}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: Enhanced LangChain + {self.model_name}\n")
            f.write(f"–†–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ: {len(self.regulations_summary)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"{'='*60}\n\n")
            f.write(contract_text)
        
        print(f"üíæ –¢–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {text_file}")
        
        # –ü—Ä–µ–≤—å—é
        print(f"\nüìÑ –ü–†–ï–í–¨–Æ –î–û–ì–û–í–û–†–ê:")
        print("-" * 60)
        preview = contract_text[:800]
        print(preview)
        if len(contract_text) > 800:
            print("...")
            print(f"[–ü–æ–∫–∞–∑–∞–Ω–æ 800 –∏–∑ {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤]")
        print("-" * 60)
        
        # 3. –ü—Ä—è–º–æ–π –∞–Ω–∞–ª–∏–∑ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏
        print("\nü§ñ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –° –í–°–¢–†–û–ï–ù–ù–û–ô –ù–û–†–ú–ê–¢–ò–í–ù–û–ô –ë–ê–ó–û–ô...")
        print("=" * 60)
        
        llm_response = self.analyze_contract_direct(contract_text)
        
        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = {
            'contract_file': os.path.basename(contract_path),
            'contract_text_file': text_file,
            'analysis_date': timestamp,
            'text_length': len(contract_text),
            'analyzer': f"Enhanced LangChain + {self.model_name}",
            'model_name': self.model_name,
            'regulations_embedded': True,
            'regulations_size': len(self.regulations_summary),
            'llm_analysis': llm_response,
            'extraction_method': 'OCR' if '=== –°—Ç—Ä–∞–Ω–∏—Ü–∞' in contract_text else 'Standard'
        }
        
        # 5. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.print_results(report)
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç—ã
        report_file = f"enhanced_analysis_{contract_name}_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        summary_file = f"enhanced_summary_{contract_name}_{timestamp}.txt"
        self.create_summary_report(report, summary_file)
        
        print(f"\nüìÑ JSON –æ—Ç—á–µ—Ç: {report_file}")
        print(f"üìã –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_file}")
        
        return report
    
    def print_results(self, report):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\n‚öñÔ∏è  –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –° –í–°–¢–†–û–ï–ù–ù–´–ú–ò –†–ï–ì–õ–ê–ú–ï–ù–¢–ê–ú–ò:")
        print("=" * 70)
        
        print(f"ü§ñ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {report['analyzer']}")
        print(f"üìä –ú–æ–¥–µ–ª—å: {report['model_name']}")
        print(f"üìè –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {report['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üîç –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {report['extraction_method']}")
        print(f"üìö –†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –≤—Å—Ç—Ä–æ–µ–Ω—ã: {'–î–∞' if report['regulations_embedded'] else '–ù–µ—Ç'}")
        print(f"üìã –†–∞–∑–º–µ—Ä –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã: {report['regulations_size']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        print(f"\nüìù –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –≠–ö–°–ü–ï–†–¢–ê:")
        print("=" * 50)
        print(report['llm_analysis'])
        print("=" * 50)
    
    def create_summary_report(self, report, summary_file):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("–°–í–û–î–ù–´–ô –û–¢–ß–ï–¢ –ê–ù–ê–õ–ò–ó–ê –î–û–ì–û–í–û–†–ê –° –í–°–¢–†–û–ï–ù–ù–´–ú–ò –†–ï–ì–õ–ê–ú–ï–ù–¢–ê–ú–ò\n")
            f.write("=" * 70 + "\n\n")
            
            f.write("–û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n")
            f.write("-" * 30 + "\n")
            f.write(f"–§–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞: {report['contract_file']}\n")
            f.write(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {report['analysis_date']}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {report['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {report['analyzer']}\n")
            f.write(f"–ú–æ–¥–µ–ª—å: {report['model_name']}\n")
            f.write(f"–ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {report['extraction_method']}\n")
            f.write(f"–§–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º: {report['contract_text_file']}\n")
            f.write(f"–†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –≤—Å—Ç—Ä–æ–µ–Ω—ã: {'–î–∞' if report['regulations_embedded'] else '–ù–µ—Ç'}\n")
            f.write(f"–†–∞–∑–º–µ—Ä –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã: {report['regulations_size']:,} —Å–∏–º–≤–æ–ª–æ–≤\n\n")
            
            f.write("–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –≠–ö–°–ü–ï–†–¢–ê\n")
            f.write("-" * 30 + "\n")
            f.write(f"{report['llm_analysis']}\n\n")
            
            f.write("–¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n")
            f.write("-" * 30 + "\n")
            f.write(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {report['analysis_date']}\n")
            f.write(f"–†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: –î–∞\n")
            f.write("\n" + "=" * 70 + "\n")
            f.write("–ö–æ–Ω–µ—Ü –æ—Ç—á–µ—Ç–∞\n")

def main():
    parser = argparse.ArgumentParser(description="Enhanced Contract Analyzer —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
    parser.add_argument("pdf_file", help="PDF —Ñ–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞")
    parser.add_argument("--model", default="qwen2.5:3b-instruct", help="–ú–æ–¥–µ–ª—å Ollama")
    parser.add_argument("--regulations", default="./regulations", help="–ü–∞–ø–∫–∞ —Å —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
    
    args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
    print(f"üìÅ –§–∞–π–ª: {args.pdf_file}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {args.model}")
    
    analyzer = EnhancedContractAnalyzer(
        regulations_path=args.regulations,
        model_name=args.model
    )
    
    result = analyzer.analyze_contract(args.pdf_file)
    
    if result:
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìã –†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –º–æ–¥–µ–ª–∏")
    else:
        print(f"\n‚ùå –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏")

if __name__ == "__main__":
    main()