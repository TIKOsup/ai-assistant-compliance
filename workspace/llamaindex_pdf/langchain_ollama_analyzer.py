# langchain_ollama_analyzer.py
import os
import sys
import argparse
from datetime import datetime
import json
import warnings
from pathlib import Path

# PDF –∏ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
import pymupdf4llm
import pytesseract
from pdf2image import convert_from_path
from PIL import Image

# LangChain –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from langchain.llms import Ollama
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")
Image.MAX_IMAGE_PIXELS = None

class LangChainOllamaAnalyzer:
    def __init__(self, regulations_path="./regulations", model_name="saiga:7b-instruct"):
        self.regulations_path = regulations_path
        self.model_name = model_name
        self.llm = None
        self.embeddings = None
        self.vectorstore = None
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LangChain + Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
        print(f"ü§ñ Instruct –º–æ–¥–µ–ª—å: {model_name}")
        
        self.setup_embeddings()
        self.setup_llm()
    
    def setup_embeddings(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        try:
            # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print("‚úÖ –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            # Fallback
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            print("‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
    
    def setup_llm(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama LLM"""
        print(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama –º–æ–¥–µ–ª–∏: {self.model_name}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            
            if response.status_code != 200:
                print("‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
                sys.exit(1)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
            models = [m['name'] for m in response.json().get('models', [])]
            if self.model_name not in models:
                print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
                os.system(f"ollama pull {self.model_name}")
            
            # –°–æ–∑–¥–∞–µ–º LLM
            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
            
            self.llm = Ollama(
                model=self.model_name,
                callback_manager=callback_manager,
                temperature=0.1,
                num_ctx=4096,
                num_predict=1024,
                top_k=40,
                top_p=0.9,
                repeat_penalty=1.1
            )
            
            print("‚úÖ Ollama LLM –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
            sys.exit(1)
    
    def extract_text_with_ocr(self, pdf_path):
        """OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)"""
        print(f"üîç OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞: {os.path.basename(pdf_path)}")
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å –º–µ–Ω—å—à–∏–º DPI –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            images = convert_from_path(pdf_path, dpi=200, thread_count=4)
            all_text = []
            
            for i, image in enumerate(images):
                print(f"üî§ OCR —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}/{len(images)}...")
                
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OCR
                custom_config = r'--oem 3 --psm 6 -l rus+eng'
                text = pytesseract.image_to_string(image, config=custom_config)
                
                if text.strip():
                    all_text.append(f"=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1} ===\n{text}")
                
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                del image
            
            combined_text = "\n\n".join(all_text)
            print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return combined_text
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
            return None
    
    def smart_extract_text(self, pdf_path):
        """–£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        try:
            text = pymupdf4llm.to_markdown(pdf_path)
            if len(text.strip()) > 100:
                print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                print("‚ö†Ô∏è –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR...")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        
        return self.extract_text_with_ocr(pdf_path)
    
    def load_regulations(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É
        if os.path.exists("./chroma_db"):
            try:
                self.vectorstore = Chroma(
                    persist_directory="./chroma_db",
                    embedding_function=self.embeddings
                )
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã: {e}")
                print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –±–∞–∑—É...")
        
        documents = []
        processed_path = "./processed_regulations"
        
        if os.path.exists(processed_path):
            txt_files = [f for f in os.listdir(processed_path) if f.endswith('.txt')]
            
            for file in txt_files:
                file_path = os.path.join(processed_path, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –û—á–∏—â–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if "=" * 60 in content:
                        content = content.split("=" * 60, 1)[-1].strip()
                    
                    if len(content.strip()) > 100:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": file.replace('.txt', ''),
                                "type": "regulation",
                                "length": len(content)
                            }
                        )
                        documents.append(doc)
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {file} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file}: {e}")
        
        if not documents:
            print("‚ö†Ô∏è –†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return False
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
        print("üî™ –†–∞–∑–±–∏–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
        splits = text_splitter.split_documents(documents)
        print(f"üìù –°–æ–∑–¥–∞–Ω–æ {len(splits)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        print("üóÑÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
        try:
            self.vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory="./chroma_db"
            )
            self.vectorstore.persist()
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
            return False
    
    def create_analysis_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º retriever"""
        if not self.vectorstore:
            print("‚ùå –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞")
            return None
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Instruct –º–æ–¥–µ–ª–∏
        prompt_template = """–¢—ã - –≤–µ–¥—É—â–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –ø—Ä–∞–≤—É –∏ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏.

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ù–û–†–ú–ê–¢–ò–í–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–´–ô –î–û–ì–û–í–û–†:
{question}

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
–ü—Ä–æ–≤–µ–¥–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É.

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–†–û–í–ï–†–ö–ò:
1. –°–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏: –°–®–ê (OFAC), –ï–°, –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è, –û–û–ù
2. –í–∞–ª—é—Ç–Ω–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: –§–ó-173 "–û –≤–∞–ª—é—Ç–Ω–æ–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏"
3. –¢–æ–≤–∞—Ä—ã –¥–≤–æ–π–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è: –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§
4. –≠–∫—Å–ø–æ—Ä—Ç–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å: –§–ó-171 "–û–± —ç–∫—Å–ø–æ—Ä—Ç–Ω–æ–º –∫–æ–Ω—Ç—Ä–æ–ª–µ"
5. –ü—Ä–æ—Ç–∏–≤–æ–¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç–º—ã–≤–∞–Ω–∏—é –¥–µ–Ω–µ–≥: –§–ó-115

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
üéØ –ò–¢–û–ì–û–í–û–ï –†–ï–®–ï–ù–ò–ï: [–ü–†–ò–ù–Ø–¢–¨/–û–¢–ö–ê–ó–ê–¢–¨/–¢–†–ï–ë–£–ï–¢_–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô_–ü–†–û–í–ï–†–ö–ò]

üìã –ü–†–ê–í–û–í–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï:
[–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–æ–≤ –∏ –ø—É–Ω–∫—Ç—ã —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤]

‚ö†Ô∏è –í–´–Ø–í–õ–ï–ù–ù–´–ï –†–ò–°–ö–ò:
[–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∏—Å–∫–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Å—Ç–µ–ø–µ–Ω–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏]

üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
[–ü–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤]

üìö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:
[–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã]

–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ —Ç–æ—á–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤.
"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        try:
            # –°–æ–∑–¥–∞–µ–º retriever —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}  # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={"prompt": prompt},
                return_source_documents=True
            )
            
            print("‚úÖ –¶–µ–ø–æ—á–∫–∞ –ø—Ä–∞–≤–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–∑–¥–∞–Ω–∞")
            return qa_chain
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ø–æ—á–∫–∏: {e}")
            print("üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏...")
            
            # Fallback - —Å–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é —Ü–µ–ø–æ—á–∫—É
            try:
                retriever = self.vectorstore.as_retriever()
                qa_chain = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=retriever,
                    return_source_documents=True
                )
                print("‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Å–æ–∑–¥–∞–Ω–∞")
                return qa_chain
            except Exception as e2:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e2}")
                return None
    
    def analyze_contract(self, contract_path):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞"""
        print(f"\n{'='*80}")
        print(f"‚öñÔ∏è  LANGCHAIN + OLLAMA –ü–†–ê–í–û–í–û–ô –ê–ù–ê–õ–ò–ó –î–û–ì–û–í–û–†–ê")
        print(f"üìÑ –§–∞–π–ª: {os.path.basename(contract_path)}")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model_name}")
        print(f"{'='*80}")
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞
        contract_text = self.smart_extract_text(contract_path)
        if not contract_text:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞")
            return None
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        contract_name = Path(contract_path).stem
        
        text_file = f"contract_text_{contract_name}_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(f"–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢ –î–û–ì–û–í–û–†–ê\n")
            f.write(f"{'='*60}\n")
            f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {os.path.basename(contract_path)}\n")
            f.write(f"–î–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {datetime.now().isoformat()}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: LangChain + Ollama ({self.model_name})\n")
            f.write(f"{'='*60}\n\n")
            f.write(contract_text)
        
        print(f"üíæ –¢–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {text_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é
        print(f"\nüìÑ –ü–†–ï–í–¨–Æ –î–û–ì–û–í–û–†–ê:")
        print("-" * 60)
        preview = contract_text[:800]
        print(preview)
        if len(contract_text) > 800:
            print("...")
            print(f"[–ü–æ–∫–∞–∑–∞–Ω–æ 800 –∏–∑ {len(contract_text)} —Å–∏–º–≤–æ–ª–æ–≤]")
        print("-" * 60)
        
        # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        if not self.load_regulations():
            print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤ - –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º")
        
        # 4. –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É –∞–Ω–∞–ª–∏–∑–∞
        qa_chain = self.create_analysis_chain()
        if not qa_chain:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ü–µ–ø–æ—á–∫—É –∞–Ω–∞–ª–∏–∑–∞")
            return None
        
        # 5. –ó–∞–ø—É—Å–∫–∞–µ–º LLM –∞–Ω–∞–ª–∏–∑
        print("\nü§ñ –ó–ê–ü–£–°–ö –ü–†–ê–í–û–í–û–ì–û –ê–ù–ê–õ–ò–ó–ê –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú LLM...")
        print("=" * 60)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            query = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É:

–¢–ï–ö–°–¢ –î–û–ì–û–í–û–†–ê:
{contract_text[:5000]}  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤

–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–∏ –Ω–∞:
- –°—Ç–æ—Ä–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–∞ –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å
- –ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –∏ —Ç–æ–≤–∞—Ä—ã/—É—Å–ª—É–≥–∏
- –í–∞–ª—é—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
- –ì–µ–æ–≥—Ä–∞—Ñ–∏—é –æ–ø–µ—Ä–∞—Ü–∏–π
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
"""
            
            print("üì° –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM...")
            result = qa_chain({"query": query})
            
            llm_response = result["result"]
            source_docs = result.get("source_documents", [])
            
            print("\n‚úÖ LLM –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
            print("=" * 60)
            
        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê LLM –ê–ù–ê–õ–ò–ó–ê: {e}")
            llm_response = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
            source_docs = []
        
        # 6. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = {
            'contract_file': os.path.basename(contract_path),
            'contract_text_file': text_file,
            'analysis_date': timestamp,
            'text_length': len(contract_text),
            'analyzer': f"LangChain + Ollama",
            'model_name': self.model_name,
            'llm_analysis': llm_response,
            'source_documents': [
                {
                    'source': doc.metadata.get("source", "Unknown"),
                    'type': doc.metadata.get("type", "Unknown"),
                    'content_preview': doc.page_content[:200] + "..."
                }
                for doc in source_docs
            ],
            'regulations_used': len(source_docs),
            'extraction_method': 'OCR' if '=== –°—Ç—Ä–∞–Ω–∏—Ü–∞' in contract_text else 'Standard'
        }
        
        # 7. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.print_results(report)
        
        # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç—ã
        # JSON –æ—Ç—á–µ—Ç
        report_file = f"langchain_analysis_{contract_name}_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # –ß–∏—Ç–∞–µ–º—ã–π –æ—Ç—á–µ—Ç
        summary_file = f"legal_summary_{contract_name}_{timestamp}.txt"
        self.create_summary_report(report, summary_file)
        
        print(f"\nüìÑ JSON –æ—Ç—á–µ—Ç: {report_file}")
        print(f"üìã –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_file}")
        
        return summary_file
    
    def print_results(self, report):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\n‚öñÔ∏è  –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–ê–í–û–í–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
        print("=" * 70)
        
        print(f"ü§ñ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {report['analyzer']}")
        print(f"üìä –ú–æ–¥–µ–ª—å: {report['model_name']}")
        print(f"üìè –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {report['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üîç –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {report['extraction_method']}")
        print(f"üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {report['regulations_used']}")
        
        if report['source_documents']:
            print(f"\nüìã –û–°–ù–û–í–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:")
            for i, doc in enumerate(report['source_documents'][:3], 1):
                print(f"   {i}. {doc['source']}")
        
        print(f"\nüìù –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –≠–ö–°–ü–ï–†–¢–ê:")
        print("=" * 40)
        print(report['llm_analysis'])
        print("=" * 40)
    
    def create_summary_report(self, report, summary_file):
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∏—Ç–∞–µ–º–æ–≥–æ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("–°–í–û–î–ù–´–ô –û–¢–ß–ï–¢ –ü–†–ê–í–û–í–û–ì–û –ê–ù–ê–õ–ò–ó–ê –î–û–ì–û–í–û–†–ê\n")
            f.write("=" * 60 + "\n\n")
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            f.write("–û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n")
            f.write("-" * 30 + "\n")
            f.write(f"–§–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞: {report['contract_file']}\n")
            f.write(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {report['analysis_date']}\n")
            f.write(f"–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {report['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {report['analyzer']}\n")
            f.write(f"–ú–æ–¥–µ–ª—å: {report['model_name']}\n")
            f.write(f"–ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {report['extraction_method']}\n")
            f.write(f"–§–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º: {report['contract_text_file']}\n\n")
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if report['source_documents']:
                f.write("–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò\n")
                f.write("-" * 40 + "\n")
                for i, doc in enumerate(report['source_documents'], 1):
                    f.write(f"{i}. {doc['source']}\n")
                f.write("\n")
            
            # –ê–Ω–∞–ª–∏–∑ LLM
            f.write("–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –≠–ö–°–ü–ï–†–¢–ê\n")
            f.write("-" * 30 + "\n")
            f.write(f"{report['llm_analysis']}\n\n")
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            f.write("–¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n")
            f.write("-" * 30 + "\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {report['regulations_used']}\n")
            f.write(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {report['analysis_date']}\n")
            f.write("\n" + "=" * 60 + "\n")
            f.write("–ö–æ–Ω–µ—Ü –æ—Ç—á–µ—Ç–∞\n")

def mainLangChain(pdf_file):
    # parser = argparse.ArgumentParser(description="LangChain + Ollama –ø—Ä–∞–≤–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–≥–æ–≤–æ—Ä–æ–≤")
    # parser.add_argument("pdf_file", help="PDF —Ñ–∞–π–ª –¥–æ–≥–æ–≤–æ—Ä–∞")
    # parser.add_argument("--model", default="saiga:7b", help="–ú–æ–¥–µ–ª—å Ollama")
    # parser.add_argument("--regulations", default="./regulations", help="–ü–∞–ø–∫–∞ —Å —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞–º–∏")
    
    # args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ LangChain + Ollama –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
    print(f"üìÅ –§–∞–π–ª: {pdf_file}")
    # print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model}")
    
    analyzer = LangChainOllamaAnalyzer(
        # regulations_path=args.regulations,
        model_name='qwen2.5:3b-instruct'
    )
    
    result = analyzer.analyze_contract(pdf_file)
    
    if result:
        print(f"\n‚úÖ –ü—Ä–∞–≤–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª–∞—Ö –æ—Ç—á–µ—Ç–æ–≤"+result)
        return result
    else:
        print(f"\n‚ùå –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏")

if __name__ == "__main__":
    mainLangChain()

__all__ = ['mainLangChain', 'LangChainOllamaAnalyzer']